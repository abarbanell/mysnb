{
  "metadata" : {
    "name" : "lgdata",
    "user_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : null,
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null
  },
  "cells" : [ {
    "metadata" : {
      "id" : "CC35D7BC14164BC4965235A97B98F156"
    },
    "cell_type" : "markdown",
    "source" : "#LG Data\nExtract data from limitless garden server via API (https: lg.dokku.abarbanell.de) and analyse with spark.\n\n## API\nthe API is still work in progress in the repo [abarbanell/limitless-garden](https:github.com/abarbanell/limitless-garden) and not all necessary API calls are yet available. \n\nThe following API is new and working, but not much data points yet: \n\n- GET /api/sensors/\n- GET /api/sensors/:id\n\nThe following API calls are not yet available: \n\n- GET /api/sensors/:id/data\n\nWe do have the old collection API\n\n- GET /api/collections/sensor (with paging and offset), only this call has currently data.\n\nAuthentication: you need an user_key which you can retrieve from threescale, but we have baked a demo key in here which allows a restriced amount of calls per minute (approx 10/min). In development we can bypass the authentication with the magic API key \"true\", but this is only enabled in dev.\n\n"
  }, {
    "metadata" : {
      "id" : "BE4967247E91410F9BC69F16D0AB3A77"
    },
    "cell_type" : "markdown",
    "source" : "## constants"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "1C6B5A49D1B643D1839CF6DC84083A77"
    },
    "cell_type" : "code",
    "source" : "// this userKey is rate-limited\nval userKey = \"6ed4688d8d487a95f4aec4f2136b04ae\"\nval baseHost = \"https://lg.dokku.abarbanell.de\"\nval baseUrl = \"/api/collections/sensor\"\nval queryString = \"?limit=10000&user_key=\" + userKey\nval fullUrl = baseHost + baseUrl  + queryString\nval lgDataDir = \"lg.d\"\n",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "6FCFC762CB5F47C588B1A7CE0D95A4CC"
    },
    "cell_type" : "markdown",
    "source" : "## some functions"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "E8072DFD201342FE810E0B3423FBE3DE"
    },
    "cell_type" : "code",
    "source" : "// this gets a file from url and saves as text\ndef getFileFromURL (url:String, dir:String) : Boolean = {\n  println(\"get from url: \"+ url)\n  val hadoopConf = sc.hadoopConfiguration\n  val fs = org.apache.hadoop.fs.FileSystem.get(hadoopConf)\n  if (fs.exists(new org.apache.hadoop.fs.Path(dir))) {\n    return true\n  } else {\n    val str = scala.io.Source.fromURL(url).mkString\n    val list = str.split(\"\\n\").filter(_ != \"\")\n    println(list.toString())\n    val rdds = sc.parallelize(list)\n    rdds.saveAsTextFile(dir)\n    return true\n  }\n  return false\n}",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "4D1100BA256748FA8030202E184F52CD"
    },
    "cell_type" : "code",
    "source" : "def deleteFile(dir: String): Boolean = {\n  println(\"delete data dir: \"+ dir)\n  val hc = sc.hadoopConfiguration\n  val fs = org.apache.hadoop.fs.FileSystem.get(hc)\n  val p = new org.apache.hadoop.fs.Path(dir)\n  if (fs.exists(p)) {\n    fs.delete(p,true)\n    return true \n  } else { \n    return false\n  }\n}",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "84C6C90EBCA24C5F881F59CCB2C6D9D5"
    },
    "cell_type" : "markdown",
    "source" : "## get data\nalways fresh data so we delete data dir before fetching.\n"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "output_stream_collapsed" : true,
      "collapsed" : false,
      "id" : "A5DAB7255E7042A2AC4C6CCE4D831828"
    },
    "cell_type" : "code",
    "source" : "//deleteFile(lgDataDir)\n\norg.apache.hadoop.fs.FileSystem.get(sc.hadoopConfiguration).delete(new org.apache.hadoop.fs.Path(lgDataDir), true)\n",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "0CD08C60D4094D9987335B928949A06C"
    },
    "cell_type" : "code",
    "source" : "// need to handle API key in querystring of url: getFileFromURL(fullUrl, lgdataDir)\ngetFileFromURL(fullUrl, lgDataDir)\n",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "81FEB979E19445308A39C38539824992"
    },
    "cell_type" : "markdown",
    "source" : "We may have now the text in our hdfs file. Now get it into a text rdd\n"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "97D647F80E9E4E908F5B2271E31190C7"
    },
    "cell_type" : "code",
    "source" : "val inputfile = sc.textFile(lgDataDir)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "B214239DF8A249AEB47A1179730FB466"
    },
    "cell_type" : "markdown",
    "source" : "look at the file content"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "778B2086431E42D1A053B79A9217DBB5"
    },
    "cell_type" : "code",
    "source" : "// inputfile.take(1).foreach( { line => println(line) } )",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "1DF4900255294F4080F2DD00EB4D7845"
    },
    "cell_type" : "markdown",
    "source" : "We now got one long line with a json array, limited by the default limit 10 objects per request (can be changed via query string parameters...) \n"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "809F2A59677C4F7F91ED2A9DA92A95BC"
    },
    "cell_type" : "code",
    "source" : "val df = sparkSession.read.json(inputfile)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "B53404F72519468384499BFDDEF85C08"
    },
    "cell_type" : "markdown",
    "source" : "# EDA\nwe now have the data in a data frame, now lets try some  analysis.\n\n- how many rows? \n- which fields exist? \n- how many rows have each field? \n- average, max, min per field\n"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "E1D25485F67F46D9B2A87497A6F13ECF"
    },
    "cell_type" : "code",
    "source" : "df.count",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "A3230CFFFF8C498E8E590DE5B671125C"
    },
    "cell_type" : "code",
    "source" : "df.show()",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "30A9AC841C194B5EAA080DD817FAE324"
    },
    "cell_type" : "code",
    "source" : "df.filter($\"timestamp\".isNull).show()",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "235D32E65F0C47AF89508F28EDA57227"
    },
    "cell_type" : "code",
    "source" : "val dfrpi02 = df.where(\"timestamp is not null and host = 'rpi02'\")",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "8628EDFCFC164BA98D9F3D6F5658BFBF"
    },
    "cell_type" : "code",
    "source" : "df.groupBy(\"host\").count().show()",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "6DDD83EF834348F1AF9FC0DDE8862C86"
    },
    "cell_type" : "code",
    "source" : "df.printSchema()",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [\n    \"temperature\"\n  ],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"List Unique Values\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "50A21939F8C34C6CB9DB450341D7A23E"
    },
    "cell_type" : "code",
    "source" : "ScatterChart(dfrpi02, fields=Some((\"timestamp\", \"temperature\")))",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}",
        "tabs_state" : "{\n  \"tab_id\": \"#tab1319103549-0\"\n}"
      },
      "id" : "6F2100E48EB244DD872C7AD0D84F4278"
    },
    "cell_type" : "code",
    "source" : "\ncase class Temperature(host: String, timestamp: java.sql.Timestamp, temperature: Double)\n",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "1025C632819746A8AB3059C99A2DCBE7"
    },
    "cell_type" : "markdown",
    "source" : "Note: only the collect() really gets the data converted to an array of Temperature records. Without this the type is still the oiginal untyped dataset.\n"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "D5DF7C5F0B964CC684C71A954F59C871"
    },
    "cell_type" : "code",
    "source" : "val temps = dfrpi02.as[Temperature].collect()",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "0B05288240FC4C84863A244FA3C0A9D6"
    },
    "cell_type" : "code",
    "source" : "TimeseriesChart(temps, fields=Some((\"timestamp\", \"temperature\")), maxPoints=10000, tickFormat=\"%Y-%m-%d\")",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "E13DB6B747DB40E385618000855CA7C2"
    },
    "cell_type" : "markdown",
    "source" : "## Typed Data Sets\ntry to create a typed data set with a case class.\n"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "A5BDF27B6D32459FB271F30C7052C5A8"
    },
    "cell_type" : "code",
    "source" : "case class Person(name: String, zip: Long)\nval df = sqlContext.read.json(sc.parallelize(\"\"\"{\"zip\": 94709, \"name\": \"Michael\"}\"\"\" :: Nil))\ndf.as[Person].collect()",
    "outputs" : [ ]
  } ],
  "nbformat" : 4
}
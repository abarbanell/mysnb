{
  "metadata" : {
    "name" : "hdfs",
    "user_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : null,
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null
  },
  "cells" : [ {
    "metadata" : {
      "id" : "03042A27CF584B08949726469C7A7575"
    },
    "cell_type" : "markdown",
    "source" : "# HDFS operations\ntry to do some general hdfs operations out of scala notebook, like reading files into Spark RDD's, saving files, listing files, removing files.\n\nWe are using latest stable hadoop (currently 2.7.2) here and use the documentation from https://hadoop.apache.org/docs/stable/api/\n\n\nFirst, let's define a filename and get a handle on the hdfs filesystem\n"
  }, {
    "metadata" : {
      "id" : "F7F2E35705DE4D619EEF92626AE43B8F"
    },
    "cell_type" : "markdown",
    "source" : "here, we get the filesystem based on the hadoop conf object - in a spark notebook this is initially a local filesystem\n"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "2621CFBC5685443889CF2A0DD5798193"
    },
    "cell_type" : "code",
    "source" : "val hadoopConf = sc.hadoopConfiguration",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "hadoopConf: org.apache.hadoop.conf.Configuration = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 1
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "3BD411057DD24EDBA04A962A19FD56D6"
    },
    "cell_type" : "code",
    "source" : "val hdfs = org.apache.hadoop.fs.FileSystem.get(hadoopConf)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "hdfs: org.apache.hadoop.fs.FileSystem = org.apache.hadoop.fs.LocalFileSystem@1a80dcbc\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 2
    } ]
  }, {
    "metadata" : {
      "id" : "C4CFE12E20A248128C5D5E3BD54AFD93"
    },
    "cell_type" : "markdown",
    "source" : "## list files"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "B262759B5C134CE3A15198557DC52107"
    },
    "cell_type" : "code",
    "source" : "val dataPath = new org.apache.hadoop.fs.Path(\"./data\")",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "dataPath: org.apache.hadoop.fs.Path = data\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 3
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "9E23A70488414C998E635F2BCE64404A"
    },
    "cell_type" : "code",
    "source" : "// get an iterator on the files\nval iter = hdfs.listFiles(dataPath, false)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "iter: org.apache.hadoop.fs.RemoteIterator[org.apache.hadoop.fs.LocatedFileStatus] = org.apache.hadoop.fs.FileSystem$6@1b943c0d\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 15
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "C537F3246D304AF1A8003CBB1F893C32"
    },
    "cell_type" : "code",
    "source" : "while (iter.hasNext()) {\n  val fileStatus = iter.next()\n  println(fileStatus.toString())\n}\n",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "LocatedFileStatus{path=file:/opt/docker/data/test.txt; isDirectory=false; length=0; replication=1; blocksize=33554432; modification_time=1467489604000; access_time=0; owner=root; group=root; permission=rw-r--r--; isSymlink=false}\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 16
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "82B7836CF1B049588470E64703C4D043"
    },
    "cell_type" : "code",
    "source" : "val it = Iterator(1, 2, 3)\nit.foreach(println)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "1\n2\n3\nit: Iterator[Int] = empty iterator\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 6
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "40A6D10CE9134BAFAD1D72D220BDB38B"
    },
    "cell_type" : "code",
    "source" : "",
    "outputs" : [ ]
  } ],
  "nbformat" : 4
}
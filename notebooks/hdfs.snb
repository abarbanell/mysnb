{
  "metadata" : {
    "name" : "hdfs",
    "user_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : null,
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null
  },
  "cells" : [ {
    "metadata" : {
      "id" : "03042A27CF584B08949726469C7A7575"
    },
    "cell_type" : "markdown",
    "source" : "# HDFS operations\ntry to do some general hdfs operations out of scala notebook\n\nWe are using latest stable hadoop (currently 2.7.2) here and use the documentation from https://hadoop.apache.org/docs/stable/api/\n\n\nFirst, let's define a filename and get a handle on the hdfs filesystem\n"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "DA8A4F1D22C14D1AA4914CCE994C2052"
    },
    "cell_type" : "code",
    "source" : "val infilepath = \"/opt/docker/data/newfile\"",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "infilepath: String = /opt/docker/data/newfile\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 21
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "2621CFBC5685443889CF2A0DD5798193"
    },
    "cell_type" : "code",
    "source" : "val hadoopConf = sc.hadoopConfiguration",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "hadoopConf: org.apache.hadoop.conf.Configuration = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 23
    } ]
  }, {
    "metadata" : {
      "id" : "F7F2E35705DE4D619EEF92626AE43B8F"
    },
    "cell_type" : "markdown",
    "source" : "here, we get the filesystem based on the hadoop conf object - in a spark notebook this is initially a local filesyste\n"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "3BD411057DD24EDBA04A962A19FD56D6"
    },
    "cell_type" : "code",
    "source" : "val hdfs = org.apache.hadoop.fs.FileSystem.get(hadoopConf)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "hdfs: org.apache.hadoop.fs.FileSystem = org.apache.hadoop.fs.LocalFileSystem@422f5e62\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 33
    } ]
  }, {
    "metadata" : {
      "id" : "C4CFE12E20A248128C5D5E3BD54AFD93"
    },
    "cell_type" : "markdown",
    "source" : "  - list files"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "B262759B5C134CE3A15198557DC52107"
    },
    "cell_type" : "code",
    "source" : "val dataPath = new org.apache.hadoop.fs.Path(\"./data\")",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "dataPath: org.apache.hadoop.fs.Path = /opt/docker/data\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 26
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "9E23A70488414C998E635F2BCE64404A"
    },
    "cell_type" : "code",
    "source" : "// get an iterator on the files\nhdfs.listFiles(dataPath, false)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res52: org.apache.hadoop.fs.RemoteIterator[org.apache.hadoop.fs.LocatedFileStatus] = org.apache.hadoop.fs.FileSystem$6@5a49adc1\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "org.apache.hadoop.fs.FileSystem$6@5a49adc1"
      },
      "output_type" : "execute_result",
      "execution_count" : 34
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "C537F3246D304AF1A8003CBB1F893C32"
    },
    "cell_type" : "code",
    "source" : "",
    "outputs" : [ ]
  } ],
  "nbformat" : 4
}